# -*- coding: utf-8 -*-
"""Tarea Kafka.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mY9Rf7-ZQ_zz4e4f1VXKI25kZm4YruD7

# Instalación de paquetes
"""

print("Installing the tensorflow-io-nightly package !")
!pip install tensorflow-io-nightly

print("Installing the kafka-python package !")
!pip install kafka-python

"""# Listado de paquetes de importación"""

import os
from datetime import datetime
import time
import threading
import json
from kafka import KafkaProducer
from kafka.errors import KafkaError
from sklearn.model_selection import train_test_split
import pandas as pd
import tensorflow as tf
import tensorflow_io as tfio

print("tensorflow-io version: {}".format(tfio.__version__))
print("tensorflow version: {}".format(tf.__version__))

"""# Descargar y configuración de Kafka"""

!curl -sSOL http://packages.confluent.io/archive/5.4/confluent-community-5.4.1-2.12.tar.gz
!tar -xzf confluent-community-5.4.1-2.12.tar.gz

"""Usamos los archivos de configuración provistos por Confluent Package"""

!cd confluent-5.4.1 && bin/zookeeper-server-start -daemon etc/kafka/zookeeper.properties
!cd confluent-5.4.1 && bin/kafka-server-start -daemon etc/kafka/server.properties
!echo "Waiting for 10 secs until kafka and zookeeper services are up and running"
!sleep 10

"""Estas instancias han creado un demonio (Procesos Java) los cuales podemos visualizar con el siguiente comando"""

!ps -ef | grep kafka

"""Ahora crearemos un topico de Kafka con las siguientes especificaciones:
```
#RFCE-train: partitions=1, replication-factor=1
#RFCE-test: partitions=2, replication-factor=1
```
"""

!confluent-5.4.1/bin/kafka-topics --create --zookeeper 127.0.0.1:2181 --replication-factor 1 --partitions 1 --topic RFCE-train
!confluent-5.4.1/bin/kafka-topics --create --zookeeper 127.0.0.1:2181 --replication-factor 1 --partitions 2 --topic RFCE-test

"""Describiremos el topico con los detalle de la configuración."""

!confluent-5.4.1/bin/kafka-topics --bootstrap-server 127.0.0.1:9092 --describe --topic RFCE-train
!confluent-5.4.1/bin/kafka-topics --bootstrap-server 127.0.0.1:9092 --describe --topic RFCE-test

"""# Iniciando la carga de datos y exploración del dataset"""

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth  
from oauth2client.client import GoogleCredentials
from google.colab import drive
drive.mount('/content/drive')

"""#aca se puede editar el modelo y colocar cualquier red neuronal"""



path = '/content/drive/My Drive/Adquisicion/MONGO_SPARK/Kafka-Ejercicios (1)/Kafka/PrediccionPm2.5-MLP/datasets'
os.chdir(path)
df = pd.read_csv('TestData.txt')

df.describe()

df.head()

print(df)

"""Limpieza de datos"""

df.dropna(subset=['pm2.5'], axis=0, inplace=True)
df.reset_index(drop=True, inplace=True)

print(df)

import datetime
df['datetime'] = df[['year', 'month', 'day', 'hour']].apply(lambda row: datetime.datetime(year=row['year'], month=row['month'], day=row['day'],
                                                                                          hour=row['hour']), axis=1)
df.sort_values('datetime', ascending=True, inplace=True)

"""Visualización de los datos"""

from matplotlib import pyplot as plt
import seaborn as sns
plt.figure(figsize=(5.5, 5.5))
g = sns.boxplot(df['pm2.5'])
g.set_title('Box plot of pm2.5')

plt.figure(figsize=(16, 5.5))
g = sns.lineplot(df['datetime'],df['pm2.5'])
g.set_title('Serie de tiempo de pm2.5')
g.set_xlabel('Index')
g.set_ylabel('Valores horarios de pm2.5')
plt.savefig('B07887_05_10.png', format='png', dpi=300)

"""Agrega una columna con los datos ajustados"""

import numpy as np
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
df['scaled_pm2.5'] = scaler.fit_transform(np.array(df['pm2.5']).reshape(-1, 1))

"""Ahora visualizamos los conjunto de datos"""

# Utilizamos pandas para realizar este proceso. 
split_date = datetime.datetime(year=2014, month=11, day=1, hour=0)
df_train = df.loc[df['datetime']<split_date]
df_test = df.loc[df['datetime']>=split_date]
print('Shape of train:', df_train.shape)
print('Shape of test:', df_test.shape)

df_train.head()

df_test.head()

#Vamos a resetar los indices para ser ordenados 
df_test.reset_index(drop=True, inplace=True)

def makeXy(ts, nb_timesteps):
    """
    Input: 
           ts: original time series
           nb_timesteps: number of time steps in the regressors
    Output: 
           X: 2-D array of regressors
           y: 1-D array of target 
    """
    X = []
    y = []
    for i in range(nb_timesteps, ts.shape[0]):
        X.append(list(ts.loc[i-nb_timesteps:i-1]))
        y.append(ts.loc[i])
    X, y = np.array(X), np.array(y)
    return X, y

x_train, y_train = makeXy(df_train['scaled_pm2.5'], 7)
print('Shape of train arrays:', x_train.shape, y_train.shape)

print(x_train)
print(y_train)

x_test, y_test = makeXy(df_test['scaled_pm2.5'], 7)
print('Shape of validation arrays:', x_test.shape, y_test.shape)

x_train_df = pd.DataFrame(data=x_train)
y_train_df = pd.DataFrame(data=y_train)
x_test_df = pd.DataFrame(data=x_test)
y_test_df = pd.DataFrame(data=y_test)

"""#Preparación de datos para enviarlos al Kafka"""

x_train = list(filter(None, x_train_df.to_csv(index=False).split("\n")[1:]))
y_train = list(filter(None, y_train_df.to_csv(index=False).split("\n")[1:]))

x_test = list(filter(None, x_test_df.to_csv(index=False).split("\n")[1:]))
y_test = list(filter(None, y_test_df.to_csv(index=False).split("\n")[1:]))

def error_callback(exc):
    raise Exception('Error while sendig data to kafka: {0}'.format(str(exc)))

def write_to_kafka(topic_name, items):
  count=0
  producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'])
  for message, key in items:
    producer.send(topic_name, key=key.encode('utf-8'), value=message.encode('utf-8')).add_errback(error_callback)
    count+=1
  producer.flush()
  print("Wrote {0} messages into topic: {1}".format(count, topic_name))

write_to_kafka("RFCE-train", zip(x_train, y_train))
write_to_kafka("RFCE-test", zip(x_test, y_test))

NUM_COLUMNS = len(x_train_df.columns)
len(x_train), len(y_train), len(x_test), len(y_test)
print(NUM_COLUMNS)

def decode_kafka_item(item):
  message = tf.io.decode_csv(item.message, [[0.0] for i in range(NUM_COLUMNS)])
  key = tf.strings.to_number(item.key)
  return (message, key)

BATCH_SIZE=64
SHUFFLE_BUFFER_SIZE=64
train_ds = tfio.IODataset.from_kafka('RFCE-train', partition=0, offset=0)
train_ds = train_ds.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)
train_ds = train_ds.map(decode_kafka_item)
train_ds = train_ds.batch(BATCH_SIZE)

# Set the parameters

OPTIMIZER="adam"
LOSS=tf.keras.losses.BinaryCrossentropy(from_logits=True)
METRICS=['accuracy']
EPOCHS=20

from keras.layers import Dense
from keras.layers import Input
from keras.layers import Dropout
from keras.layers import Flatten
from keras.layers.convolutional import ZeroPadding1D
from keras.layers.convolutional import Conv1D
from keras.layers.pooling import AveragePooling1D
from keras.optimizers import SGD
from keras.models import Model
from keras.models import load_model
from keras.callbacks import ModelCheckpoint

input_layer = Input(shape=(7,1), dtype='float32')
zeropadding_layer = ZeroPadding1D(padding=1)(input_layer)
conv1D_layer1 = Conv1D(64, 3, strides=1, use_bias=True)(zeropadding_layer)
conv1D_layer2 = Conv1D(32, 3, strides=1, use_bias=True)(conv1D_layer1)
avgpooling_layer = AveragePooling1D(pool_size=3, strides=1)(conv1D_layer2)
flatten_layer = Flatten()(avgpooling_layer)
dense_layer1 = Dense(32)(avgpooling_layer)
dense_layer2 = Dense(16)(dense_layer1)
dropout_layer = Dropout(0.2)(flatten_layer)
output_layer = Dense(1, activation='linear')(dropout_layer)

model = Model(inputs=input_layer, outputs=output_layer)
model.compile(loss='mean_absolute_error', optimizer='adam')#SGD(lr=0.001, decay=1e-5))
model.summary()

# compile the model
model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)

# fit the model
model.fit(train_ds, epochs=20)

test_ds = tfio.experimental.streaming.KafkaGroupIODataset(
    topics=["RFCE-test"],
    group_id="testcg",
    servers="127.0.0.1:9092",
    stream_timeout=10000,
    configuration=[
        "session.timeout.ms=7000",
        "max.poll.interval.ms=8000",
        "auto.offset.reset=earliest"
    ],
)

def decode_kafka_test_item(raw_message, raw_key):
  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])
  key = tf.strings.to_number(raw_key)
  return (message, key)

test_ds = test_ds.map(decode_kafka_test_item)
test_ds = test_ds.batch(BATCH_SIZE)

res = model.evaluate(test_ds)
print("test loss, test acc:", res)

!confluent-5.4.1/bin/kafka-consumer-groups --bootstrap-server 127.0.0.1:9092 --describe --group testcg